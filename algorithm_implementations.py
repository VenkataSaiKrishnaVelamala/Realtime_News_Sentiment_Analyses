# -*- coding: utf-8 -*-
"""Algorithm_Implementations.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hjmWcxSYFeogbQHMpppdJma_x__sip6W
"""

# Working Reservoir Sampling

from pyflink.common.serialization import DeserializationSchema, SimpleStringSchema
from pyflink.common.typeinfo import Types
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.connectors import FlinkKafkaConsumer, FlinkKafkaProducer

import json
import random

class JsonDeserializationSchema(DeserializationSchema):
    def deserialize(self, message: bytes):
        try:
            json_data = json.loads(message.decode('utf-8'))
            return True, json_data
        except json.JSONDecodeError:
            return False, None

    def get_produced_type(self):
        return Types.PICKLED_BYTE_ARRAY()

def main():
    env = StreamExecutionEnvironment.get_execution_environment()
    env.set_parallelism(1)
    env.add_jars("file:///Users/spartan/Downloads/flink-sql-connector-kafka-1.17.2.jar")

    kafka_consumer = FlinkKafkaConsumer(
        topics='Big_Data_Project',
        deserialization_schema=SimpleStringSchema(),
        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'Big_Data_Project_1'}
    )

    data_stream = env.add_source(kafka_consumer)

    def json_parse_map(value):
        json_data = json.loads(value)
        return json_data

    def reservoir_sampling(data, sample_size=15):
        sample = []
        for index, item in enumerate(data):
            if index < sample_size:
                sample.append(item)
            else:
                r = random.randint(0, index)
                if r < sample_size:
                    sample[r] = item
        return sample

    parsed_stream = data_stream.map(json_parse_map)
    sampled_stream = parsed_stream.map(lambda x: reservoir_sampling([x]))
    sampled_stream.print()
    #kafka_producer = FlinkKafkaProducer(
    #    topic='per_test',
    #    serialization_schema=SimpleStringSchema(),
    #    producer_config={'bootstrap.servers': 'localhost:9092'}
    #)
    print(sampled_stream)
    # sampled_stream.add_sink(kafka_producer)

    env.execute("Reservoir Sampling with Kafka and PyFlink")

if __name__ == "__main__":
    main()

# Working Min-Hash Algorithm

from pyflink.datastream import StreamExecutionEnvironment
from pyflink.common.serialization import SimpleStringSchema
import json
import random
import hashlib

def main():
    env = StreamExecutionEnvironment.get_execution_environment()
    env.set_parallelism(1)
    env.add_jars("file:///Users/spartan/Downloads/flink-sql-connector-kafka-1.17.2.jar")

    kafka_consumer = FlinkKafkaConsumer(
        topics='Big_Data_Project_LSH',
        deserialization_schema=SimpleStringSchema(),
        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'Big_Data_Project_LSH_1'}
    )

    data_stream = env.add_source(kafka_consumer)

    def json_parse_map(value):
        try:
            json_data = json.loads(value)
            return json_data['title'], json_data['content']
        except Exception as e:
            print(f"Failed to parse JSON: {e}")
            return None
    def get_min_hash(text, num_hashes):
        min_hashes = [float('inf')] * num_hashes
        for word in set(text.split()):
            for i in range(num_hashes):
                hash_val = int(hashlib.sha256(f"{word}_{i}".encode()).hexdigest(), 16)
                min_hashes[i] = min(min_hashes[i], hash_val)
        return min_hashes

    def lsh_bucketing(title, description, num_hashes, num_bands):
        min_hashes = get_min_hash(description, num_hashes)
        buckets = []
        band_size = len(min_hashes) // num_bands
        for i in range(num_bands):
            band = min_hashes[i * band_size:(i + 1) * band_size]
            bucket_hash = hashlib.sha256(str(band).encode()).hexdigest()
            buckets.append((title, bucket_hash))  # Pair the title with the bucket hash
        return buckets

    parsed_stream = data_stream.map(json_parse_map)
    bucketed_stream = parsed_stream.flat_map(lambda x: lsh_bucketing(x[0], x[1], 10, 1))

    bucketed_stream.print()

    env.execute("Locality-Sensitive Hashing with Kafka and PyFlink")

if __name__ == "__main__":
    main()

