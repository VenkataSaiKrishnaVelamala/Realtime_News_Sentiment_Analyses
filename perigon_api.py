# -*- coding: utf-8 -*-
"""Perigon_API.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19q0AIrHCu32853UJoBpIKlo_Qe7otwDZ
"""

# Fetching the data

import requests
import json

API_KEY = "1ff5fc7d-f9e5-4e98-9edc-37661cbeae2d"
ALL_URL_1 = f"https://api.goperigon.com/v1/all?category=Business&sourceGroup=top100&showReprints=false&from=2024-04-20&to=2024-04-27&apiKey={API_KEY}"
ALL_URL_2 = f"https://api.goperigon.com/v1/all?category=Sports&sourceGroup=top100&showReprints=false&from=2024-04-20&to=2024-04-27&apiKey={API_KEY}"
ALL_URL_3 = f"https://api.goperigon.com/v1/all?q=Crime&sourceGroup=top100&showReprints=false&from=2024-04-20&to=2024-04-27&apiKey={API_KEY}"
ALL_URL_4 = f"https://api.goperigon.com/v1/all?q=Transportation&sourceGroup=top100&showReprints=false&from=2024-04-20&to=2024-04-27&apiKey={API_KEY}"
ALL_URL_5 = f"https://api.goperigon.com/v1/all?q=Inflation&sourceGroup=top100&showReprints=false&from=2024-04-20&to=2024-04-27&apiKey={API_KEY}"
ALL_URL_6 = f"https://api.goperigon.com/v1/all?q=Layoffs&sourceGroup=top100&showReprints=false&from=2024-04-20&to=2024-04-27&apiKey={API_KEY}"
ALL_URL_7 = f"https://api.goperigon.com/v1/all?q=Stocks&sourceGroup=top100&showReprints=false&from=2024-04-20&to=2024-04-27&apiKey={API_KEY}"
ALL_URL_8 = f"https://api.goperigon.com/v1/all?q=Food&sourceGroup=top100&showReprints=false&from=2024-04-20&to=2024-04-27&apiKey={API_KEY}"
ALL_URL_9 = f"https://api.goperigon.com/v1/all?q=Health&sourceGroup=top100&showReprints=false&from=2024-04-20&to=2024-04-27&apiKey={API_KEY}"
ALL_URL_10 = f"https://api.goperigon.com/v1/all?q=Fashion&sourceGroup=top100&showReprints=false&from=2024-04-20&to=2024-04-27&apiKey={API_KEY}"
ALL_URL_11 = f"https://api.goperigon.com/v1/all?q=Travel&sourceGroup=top100&showReprints=false&from=2024-04-20&to=2024-04-27&apiKey={API_KEY}"
ALL_URL_12 = f"https://api.goperigon.com/v1/all?q=Technology&sourceGroup=top100&showReprints=false&from=2024-04-20&to=2024-04-27&apiKey={API_KEY}"
ALL_URL_13 = f"https://api.goperigon.com/v1/all?q=Entertainment&sourceGroup=top100&showReprints=false&from=2024-04-20&to=2024-04-27&apiKey={API_KEY}"
ALL_URL_14 = f"https://api.goperigon.com/v1/all?q=Science&sourceGroup=top100&showReprints=false&from=2024-04-20&to=2024-04-27&apiKey={API_KEY}"
ALL_URL_15 = f"https://api.goperigon.com/v1/all?q=Politics&sourceGroup=top100&showReprints=false&from=2024-04-20&to=2024-04-27&apiKey={API_KEY}"



# resp = requests.get(f"{ALL_URL}&state=CA")

l = []

resp1 = requests.get(f"{ALL_URL_1}")
resp2 = requests.get(f"{ALL_URL_2}")
resp3 = requests.get(f"{ALL_URL_3}")
resp4 = requests.get(f"{ALL_URL_4}")
resp5 = requests.get(f"{ALL_URL_5}")
resp6 = requests.get(f"{ALL_URL_6}")
resp7 = requests.get(f"{ALL_URL_7}")
resp8 = requests.get(f"{ALL_URL_8}")
resp9 = requests.get(f"{ALL_URL_9}")
resp10 = requests.get(f"{ALL_URL_10}")
resp11 = requests.get(f"{ALL_URL_11}")
resp12 = requests.get(f"{ALL_URL_12}")
resp13 = requests.get(f"{ALL_URL_13}")
resp14 = requests.get(f"{ALL_URL_14}")
resp15 = requests.get(f"{ALL_URL_15}")



#q=inflation AND prices
#url = f"https://api.goperigon.com/v1/all?category=Business&sourceGroup=top100&showReprints=false&apiKey={API_KEY}"


article1 = resp1.json()
article2 = resp2.json()
article3 = resp3.json()
article4 = resp4.json()
article5 = resp5.json()
article6 = resp6.json()
article7 = resp7.json()
article8 = resp8.json()
article9 = resp9.json()
article10 = resp10.json()
article11 = resp11.json()
article12 = resp12.json()
article13 = resp13.json()
article14 = resp14.json()
article15 = resp15.json()


#print(article["title"])
for x in article1['articles']:
    l.append({'articleID': x['articleId'], 'title': x['title'], 'description': x['description'], 'content': x['content'], 'publishedDate': x['pubDate'], 'url': x['url']})

for x in article2['articles']:
    l.append({'articleID': x['articleId'], 'title': x['title'], 'description': x['description'], 'content': x['content'], 'publishedDate': x['pubDate'], 'url': x['url']})

for x in article3['articles']:
    l.append({'articleID': x['articleId'], 'title': x['title'], 'description': x['description'], 'content': x['content'], 'publishedDate': x['pubDate'], 'url': x['url']})

for x in article4['articles']:
    l.append({'articleID': x['articleId'], 'title': x['title'], 'description': x['description'], 'content': x['content'], 'publishedDate': x['pubDate'], 'url': x['url']})

for x in article5['articles']:
    l.append({'articleID': x['articleId'], 'title': x['title'], 'description': x['description'], 'content': x['content'], 'publishedDate': x['pubDate'], 'url': x['url']})

for x in article6['articles']:
    l.append({'articleID': x['articleId'], 'title': x['title'], 'description': x['description'], 'content': x['content'], 'publishedDate': x['pubDate'], 'url': x['url']})

for x in article7['articles']:
    l.append({'articleID': x['articleId'], 'title': x['title'], 'description': x['description'], 'content': x['content'], 'publishedDate': x['pubDate'], 'url': x['url']})

for x in article8['articles']:
    l.append({'articleID': x['articleId'], 'title': x['title'], 'description': x['description'], 'content': x['content'], 'publishedDate': x['pubDate'], 'url': x['url']})

for x in article9['articles']:
    l.append({'articleID': x['articleId'], 'title': x['title'], 'description': x['description'], 'content': x['content'], 'publishedDate': x['pubDate'], 'url': x['url']})

for x in article10['articles']:
    l.append({'articleID': x['articleId'], 'title': x['title'], 'description': x['description'], 'content': x['content'], 'publishedDate': x['pubDate'], 'url': x['url']})

for x in article11['articles']:
    l.append({'articleID': x['articleId'], 'title': x['title'], 'description': x['description'], 'content': x['content'], 'publishedDate': x['pubDate'], 'url': x['url']})

for x in article12['articles']:
    l.append({'articleID': x['articleId'], 'title': x['title'], 'description': x['description'], 'content': x['content'], 'publishedDate': x['pubDate'], 'url': x['url']})

for x in article13['articles']:
    l.append({'articleID': x['articleId'], 'title': x['title'], 'description': x['description'], 'content': x['content'], 'publishedDate': x['pubDate'], 'url': x['url']})

for x in article14['articles']:
    l.append({'articleID': x['articleId'], 'title': x['title'], 'description': x['description'], 'content': x['content'], 'publishedDate': x['pubDate'], 'url': x['url']})

for x in article15['articles']:
    l.append({'articleID': x['articleId'], 'title': x['title'], 'description': x['description'], 'content': x['content'], 'publishedDate': x['pubDate'], 'url': x['url']})

#lst = []
#for x in article['articles']:
#    lst.append({'articleID': x['articleId'], 'title': x['title'], 'description': x['description'], 'content': x['content'], 'publishedDate': x['pubDate'], 'url': x['url']})

# Test Print

for x, y in enumerate(l, 1):
    print(f"ArticleID: {y['articleID']}")
    print(f"Title: {y['title']}")
    print(f"Description: {y['description']}")
    print(f"Content: {y['content']}")
    print(f"Published Date: {y['publishedDate']}")
    print(f"URL: {y['url']}")

# Reservoir Sampling Producer Code

from confluent_kafka import Producer
import json

# Function to send data to Kafka topic
def send_to_kafka(topic, data):
    producer = Producer({'bootstrap.servers': 'localhost:9092'})  # Adjust Kafka broker address as needed
    for entry in data:
        producer.produce(topic, json.dumps(entry).encode('utf-8'))
    producer.flush()

# Main function
def main():

    # Define Kafka topic
    kafka_topic = "Big_Data_Project"  # Adjust Kafka topic name as needed

    # Send data to Kafka topic
    send_to_kafka(kafka_topic, l)

if __name__ == "__main__":
    main()

# LSH Producer Code

from confluent_kafka import Producer
import json

# Function to send data to Kafka topic
def send_to_kafka(topic, data):
    producer = Producer({'bootstrap.servers': 'localhost:9092'})  # Adjust Kafka broker address as needed
    for entry in data:
        producer.produce(topic, json.dumps(entry).encode('utf-8'))
    producer.flush()

# Main function
def main():

    # Define Kafka topic
    kafka_topic = "Big_Data_Project_LSH"  # Adjust Kafka topic name as needed

    # Send data to Kafka topic
    send_to_kafka(kafka_topic, l)

if __name__ == "__main__":
    main()











a_1 = 10
a_2 = 20


for i in range(1,3):
    print(i)
    print(f{})

